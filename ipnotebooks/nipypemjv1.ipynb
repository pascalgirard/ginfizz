{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plus slice timing + 16 mars normalisation des images structurelles segmentees en tissus\n",
    "premier notebook avec nouvelle version de marc spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nipype.interfaces.io as nio           # Data i/o\n",
    "import nipype.interfaces.spm as spm          # spm\n",
    "import nipype.interfaces.matlab as mlab      # how to run matlab\n",
    "import nipype.interfaces.utility as util     # utility\n",
    "import nipype.pipeline.engine as pe          # pypeline engine\n",
    "import nipype.algorithms.rapidart as ra      # artifact detection\n",
    "import nipype.algorithms.modelgen as model   # model specification\n",
    "import os                                    # system functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spm_standalone='/homes_unix/hirsch/historique_fli_iam/essai_spm_stand_alone/spm12/run_spm12.sh'\n",
    "mcr='/homes_unix/hirsch/historique_fli_iam/essai_spm_stand_alone/mcr2016/v91'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces import spm\n",
    "\n",
    "# essai du mercredi soir matlab_cmd = ' '.join([spm_standalone,mcr,'batch','script'])\n",
    "matlab_cmd = ' '.join([spm_standalone,mcr,'batch'])\n",
    "spm.SPMCommand.set_mlab_paths(matlab_cmd=matlab_cmd, use_mcr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nipype verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nipype\n",
    "# Optional: Use the following lines to increase verbosity of output\n",
    "nipype.config.set('logging', 'workflow_level',  'INFO')\n",
    "nipype.config.set('logging', 'interface_level', 'INFO')\n",
    "nipype.logging.update_logging(nipype.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - we decode the xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flibasedir = '/scratch/user/hirsch/datadir/data_set/t0009/repos01'\n",
    "resultdir = '/scratch/user/hirsch/datadir/data_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we get the xml file in this directory\n",
    "def getXmlFile(flibasedir):\n",
    "    import os\n",
    "    dirs = os.listdir( flibasedir )\n",
    "    #print dirs\n",
    "\n",
    "    import re\n",
    "    xmlmatch = r'(.*)xml'\n",
    "    for dir in dirs:\n",
    "        xmlfound=re.search(xmlmatch,dir,flags=0)\n",
    "        if xmlfound:\n",
    "            #print xmlfound.group()\n",
    "            return flibasedir + '/' + xmlfound.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/hirsch/datadir/data_set/t0009/repos01/xml_example.xml\n"
     ]
    }
   ],
   "source": [
    "xmlfile = getXmlFile(flibasedir)\n",
    "\n",
    "print xmlfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the we decode the xmlfile\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(xmlfile)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RS_analysis\n"
     ]
    }
   ],
   "source": [
    "print root.tag\n",
    "\n",
    "def getEpiInfos(xmlfile):\n",
    "    result = {}\n",
    "    try:\n",
    "        import xml.etree.ElementTree as ET\n",
    "        tree = ET.parse(xmlfile)\n",
    "        root = tree.getroot()\n",
    "        # epibold directory\n",
    "        for epibold in root.iter('EPIBOLD'):\n",
    "            for child in epibold:\n",
    "                print child.text\n",
    "                if child.tag == 'file':\n",
    "                    epidir = child.text\n",
    "                    result['epidir'] = epidir\n",
    "                    print epidir\n",
    "                    \n",
    "                # parameters\n",
    "                if child.tag == 'parameters':\n",
    "                    print \"parameters\"\n",
    "                    \n",
    "                    for child2 in child:\n",
    "                                                     \n",
    "                        # TR\n",
    "                        if child2.tag == 'TR':\n",
    "                            for child3 in child2:\n",
    "                                if child3.tag == 'value':\n",
    "                                    tr = child3.text\n",
    "                                    print tr\n",
    "                                    result['TR'] = tr\n",
    "                    \n",
    "                        # dynamics\n",
    "                        if child2.tag == 'dynamics':\n",
    "                            for child3 in child2:\n",
    "                                if child3.tag == 'value':\n",
    "                                    dynamics = child3.text\n",
    "                                    result['dynamics'] = dynamics\n",
    "                                    \n",
    "                        # sliceTimingVector\n",
    "                        if child2.tag == 'sliceTimingVector':\n",
    "                            for child3 in child2:\n",
    "                                if child3.tag == 'value':\n",
    "                                    sliceTimingVector = child3.text\n",
    "                                    result['sliceTimingVector'] = sliceTimingVector\n",
    "                                    \n",
    "                        # nb_slices\n",
    "                        if child2.tag == 'nb_slices':\n",
    "                            for child3 in child2:\n",
    "                                if child3.tag == 'value':\n",
    "                                    nb_slices = child3.text\n",
    "                                    result['nb_slices'] = nb_slices\n",
    "                \n",
    "        \n",
    "    except:\n",
    "        print 'exception'\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/EPIBOLD/\n",
      "/EPIBOLD/\n",
      "bold-dataset\n",
      "\n",
      "     \n",
      "parameters\n",
      "2\n",
      "{'sliceTimingVector': '1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30', 'dynamics': '240', 'nb_slices': '31', 'TR': '2', 'epidir': '/EPIBOLD/'}\n"
     ]
    }
   ],
   "source": [
    "epiResults = getEpiInfos(xmlfile)\n",
    "print epiResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getT1file(xmlfile):\n",
    "    try:\n",
    "        import xml.etree.ElementTree as ET\n",
    "        tree = ET.parse(xmlfile)\n",
    "        root = tree.getroot()\n",
    "        # T1 directory and file\n",
    "        for t1 in root.iter('T1'):\n",
    "            for child in t1:\n",
    "                #print child.text\n",
    "                if child.tag == 'file':\n",
    "                    t1 = child.text\n",
    "                    t1 = flibasedir + t1\n",
    "                    # print  t1\n",
    "    except:\n",
    "        print 'exception'\n",
    "    return t1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/hirsch/datadir/data_set/t0009/repos01/T1/t0009_t1_s03.nii\n"
     ]
    }
   ],
   "source": [
    "t1 = getT1file(xmlfile)\n",
    "print t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Pileline -> declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preproc = pe.Workflow(name='preproc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - first node data grabbing by select files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/EPIBOLD/\n",
      "/EPIBOLD/\n",
      "bold-dataset\n",
      "\n",
      "     \n",
      "parameters\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'T1': <undefined>, 'epi': <undefined>}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epidir = getEpiInfos(xmlfile)['epidir']\n",
    "epidir = flibasedir + epidir\n",
    "\n",
    "from nipype import SelectFiles, Node\n",
    "templates = dict(T1=t1,\n",
    "                 epi= epidir + \"/\" + \"*.nii\")\n",
    "\n",
    "filesource = Node(SelectFiles(templates), \"filesource\")\n",
    "filesource.inputs.subject_id = \"subj1\"\n",
    "filesource.outputs.get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - segment T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment = pe.Node(interface=spm.NewSegment(), name='segment')\n",
    "\n",
    "#seg.inputs.channel_files = 't0009_t1_s03.nii'\n",
    "segment.inputs.channel_info =(0.001,60.0,(True,True))          \n",
    "segment.inputs.sampling_distance= 3.0\n",
    "# todo donner un chemin de TPM.nii comme argument de la future fct\n",
    "tissue1 = (('/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM.nii', 1), 1, (True,False), (False, False))\n",
    "tissue2 = (('/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM.nii', 2), 1, (True,False), (False, False))\n",
    "tissue3 = (('/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM.nii', 3), 2, (True,False), (False, False))\n",
    "tissue4 = (('/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM.nii', 4), 3, (True,False), (False, False))\n",
    "tissue5 = (('/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM.nii', 5), 4, (True,False), (False, False))\n",
    "tissue6 = (('/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM.nii', 6), 2, (False,False), (False, False))\n",
    "segment.inputs.tissues = [tissue1, tissue2, tissue3, tissue4, tissue5, tissue6]\n",
    "segment.inputs.affine_regularization = 'mni'\n",
    "#segment.inputs.warping_regularization = [0.0, 0.001, 0.5, 0.05, 0.2]\n",
    "segment.inputs.write_deformation_fields = [False, False]\n",
    "\n",
    "# 2 - segment\n",
    "preproc.connect(filesource,\"T1\" ,segment, 'channel_files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 bis - image calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeStructuralImage(gm_img, wm_img, mean_img):\n",
    "    ''' \n",
    "    takes 3 images arguments: grey matter, white matter, and bias corrected T1 img.\n",
    "    compute a binary mask with gm and wm thresholded at 0.2 combined with the mean image coming out of segment.\n",
    "    used to realigne properly the EPI imgs. \n",
    "    '''\n",
    "\n",
    "    import numpy as np     \n",
    "    import nibabel as nib     \n",
    "    import os \n",
    "    \n",
    "    i1=nib.load(gm_img)         \n",
    "    i1array=np.asarray(i1.dataobj).copy() # Avoid caching the proxy image\n",
    "    \n",
    "    i2=nib.load(wm_img)         \n",
    "    i2array=np.asarray(i2.dataobj).copy()\n",
    "    \n",
    "    i3=nib.load(mean_img)         \n",
    "    i3array=np.asarray(i3.dataobj).copy()\n",
    "    hdr3 = i3.header\n",
    "    \n",
    "    gi = i1array + i2array\n",
    "    # threshold image gm + wm at 0.2\n",
    "    gi[gi < 0.2] = 0\n",
    "    # binary mask the resulting image\n",
    "    gi[gi > 0.0] = 1\n",
    "    \n",
    "    # apply the binary mask to the bias corrected T1\n",
    "    struct_image_array = np.multiply(gi, i3array)\n",
    "    \n",
    "    # crate the resulting image\n",
    "    struct_image = nib.Nifti1Image(struct_image_array, np.eye(4))\n",
    "    \n",
    "    out_file = os.path.join(os.getcwd(), 'struct_image.nii')\n",
    "    nib.save(struct_image, out_file)\n",
    "        \n",
    "    return out_file\n",
    "\n",
    "def regexfilter(files_list,patern):\n",
    "    import re\n",
    "    for f in files_list:\n",
    "        for g in f:\n",
    "            if re.search(patern, str(g)):\n",
    "                res = g\n",
    "    return res\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "res = [f for f in os.listdir(path) if re.search(r'(abc|123|a1b).*\\.txt$', f)]\n",
    "for f in res:\n",
    "    print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "\n",
    "computeStructuralImage = Node(Function(input_names=['gm_img', 'wm_img', 'mean_img'],\n",
    "                                output_names=['out_file'],\n",
    "                                function=computeStructuralImage),\n",
    "                                name='computeStructuralImage')\n",
    "\n",
    "# 2 bis - image calculator -> to ger a nice structurak images to get good registration results\n",
    "# input_names=['gm_img', 'wm_img', 'bias_corrected_img'],\n",
    "preproc.connect(segment, ('native_class_images', regexfilter,r'.*c1.*.nii') ,computeStructuralImage, 'gm_img')\n",
    "preproc.connect(segment, ('native_class_images', regexfilter,r'.*c2.*.nii') ,computeStructuralImage, 'wm_img')\n",
    "# todo retrouver eventuellement la mean image je ne sais pas pourquoi elle n est pas produite par segment\n",
    "preproc.connect(filesource,\"T1\" ,computeStructuralImage, 'mean_img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - slice timing of epibold images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30]\n"
     ]
    }
   ],
   "source": [
    "l = epiResults['sliceTimingVector'].split()\n",
    "lint =[]\n",
    "for i in l:\n",
    "    lint.append(int(i))\n",
    "print lint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nipype.interfaces.spm import SliceTiming\n",
    "\n",
    "st = pe.Node(interface=spm.SliceTiming(), name='st')\n",
    "#st.inputs.in_files = 'functional.nii'\n",
    "st.inputs.num_slices = int(epiResults['nb_slices'])\n",
    "st.inputs.time_repetition = float(epiResults['TR'])\n",
    "st.inputs.time_acquisition = float(epiResults['TR']) - float(epiResults['TR'])/2.0\n",
    "# todo check previous line\n",
    "print st.inputs.time_acquisition\n",
    "st.inputs.slice_order = lint\n",
    "st.inputs.ref_slice = int(epiResults['nb_slices'])\n",
    "#st.inputs.jobtype = 'estwrite'\n",
    "\n",
    "preproc.connect(filesource,\"epi\" ,st, 'in_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - realign of epibold images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "realign = pe.Node(interface=spm.Realign(), name='realign')\n",
    "\n",
    "realign.inputs.register_to_mean = True\n",
    "realign.inputs.quality = 0.9\n",
    "realign.inputs.separation = 4\n",
    "realign.inputs.fwhm = 5\n",
    "realign.inputs.interp = 2\n",
    "realign.inputs.wrap = [0, 0, 0]\n",
    "# essai au defaut 2, 1 realign.inputs.write_which = [0, 1]\n",
    "realign.inputs.write_which = [2, 1]\n",
    "realign.inputs.write_interp = 4\n",
    "realign.inputs.wrap = [0, 0, 0]\n",
    "realign.inputs.write_mask = True\n",
    "realign.inputs.out_prefix = 'r'\n",
    "realign.inputs.jobtype = 'estwrite'\n",
    "\n",
    "preproc.connect(st,\"timecorrected_files\" ,realign, 'in_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Coregistration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spm.Coregister()\n",
    "coregister = pe.Node(interface=spm.Coregister(), name='coregister' )\n",
    "\n",
    "coregister.inputs.cost_function = 'nmi'\n",
    "coregister.inputs.separation = [4.0, 2.0]\n",
    "coregister.inputs.jobtype = 'estwrite'\n",
    "coregister.inputs.tolerance = [0.02, 0.02, 0.02, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001]\n",
    "coregister.inputs.fwhm = [7.0, 7.0]\n",
    "coregister.inputs.write_interp = 4\n",
    "coregister.inputs.write_wrap = [0, 0, 0]\n",
    "coregister.inputs.write_mask = False \n",
    "coregister.inputs.out_prefix = 'c'\n",
    "coregister.inputs.jobtype = 'estwrite'\n",
    "\n",
    "preproc.connect(computeStructuralImage,\"out_file\" ,coregister, 'source')\n",
    "preproc.connect(realign,\"mean_image\" ,coregister, 'target')\n",
    "preproc.connect(realign,\"realigned_files\" ,coregister, 'apply_to_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# passage à normalize sans 12 non on repasse en 12 et on prend des param de pierre y\n",
    "norm12 = pe.Node(interface=spm.Normalize12(), name='norm12')\n",
    "\n",
    "norm12.inputs.bias_regularization = 0.0001\n",
    "norm12.inputs.bias_fwhm = 60\n",
    "# todo putthis parameter in args of a fct\n",
    "# norm12.inputs.tpm = '/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM.nii'\n",
    "norm12.inputs.affine_regularization_type = 'mni'\n",
    "norm12.inputs.sampling_distance = 3.0\n",
    "norm12.inputs.write_bounding_box = [[-90.0, -126.0, -72.0],[ 90.0, 90.0, 108.0]]\n",
    "norm12.inputs.write_voxel_sizes = [2.0, 2.0, 2.0]\n",
    "norm12.inputs.write_interp = 4 \n",
    "norm12.inputs.jobtype = 'estwrite'\n",
    "norm12.inputs.use_v8struct = True\n",
    "#norm12.inputs.out_prefix = 'w'\n",
    "\n",
    "preproc.connect(segment,'bias_corrected_images' ,norm12, 'image_to_align')\n",
    "preproc.connect(coregister,\"coregistered_files\" ,norm12, 'apply_to_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# deuxieme normalisation pour normaliser les masks wm et lcf\n",
    "norm12bis = pe.Node(interface=spm.Normalize12(), name='norm12bis')\n",
    "\n",
    "norm12bis.inputs.bias_regularization = 0.0001\n",
    "norm12bis.inputs.bias_fwhm = 60\n",
    "# todo putthis parameter in args of a fct\n",
    "# norm12.inputs.tpm = '/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM.nii'\n",
    "norm12bis.inputs.affine_regularization_type = 'mni'\n",
    "norm12bis.inputs.sampling_distance = 3.0\n",
    "norm12bis.inputs.write_bounding_box = [[-90.0, -126.0, -72.0],[ 90.0, 90.0, 108.0]]\n",
    "norm12bis.inputs.write_voxel_sizes = [2.0, 2.0, 2.0]\n",
    "norm12bis.inputs.write_interp = 4 \n",
    "norm12bis.inputs.jobtype = 'estwrite'\n",
    "norm12bis.inputs.use_v8struct = True\n",
    "#norm12bis.inputs.out_prefix = 'w'\n",
    "\n",
    "# rajout de normalisation des tissus\n",
    "preproc.connect(segment,'bias_corrected_images' ,norm12bis, 'image_to_align')\n",
    "preproc.connect(segment,\"native_class_images\" ,norm12bis, 'apply_to_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:workflow:['check', 'execution', 'logging']\n",
      "INFO:workflow:Running serially.\n",
      "INFO:workflow:Executing node filesource in dir: /tmp/tmpuftLcP/preproc/filesource\n",
      "INFO:workflow:Runtime memory and threads stats unavailable\n",
      "INFO:workflow:Executing node segment in dir: /tmp/tmp5nskvw/preproc/segment\n",
      "INFO:workflow:Runtime memory and threads stats unavailable\n",
      "INFO:workflow:Executing node computeStructuralImage in dir: /tmp/tmpRubAsj/preproc/computeStructuralImage\n",
      "INFO:workflow:Executing node norm12bis in dir: /tmp/tmpF4vWCG/preproc/norm12bis\n",
      "INFO:workflow:Runtime memory and threads stats unavailable\n",
      "INFO:workflow:Executing node st in dir: /tmp/tmpW_txtq/preproc/st\n",
      "INFO:workflow:Runtime memory and threads stats unavailable\n",
      "INFO:workflow:Executing node realign in dir: /tmp/tmpeL0uAp/preproc/realign\n",
      "INFO:workflow:Runtime memory and threads stats unavailable\n",
      "INFO:workflow:Executing node coregister in dir: /tmp/tmp351OOy/preproc/coregister\n",
      "INFO:workflow:Runtime memory and threads stats unavailable\n",
      "INFO:workflow:Executing node norm12 in dir: /tmp/tmpTQcoiP/preproc/norm12\n",
      "INFO:workflow:Runtime memory and threads stats unavailable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7f14b2691390>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 -fsl merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces.fsl import Merge\n",
    "fsl_merge = pe.Node(interface=Merge(), name=\"fsl_merge\")     \n",
    " \n",
    "fsl_merge.inputs.dimension = 't'\n",
    "fsl_merge.inputs.output_type = 'NIFTI_GZ'\n",
    "# todo recuperer le TR deja exploité du xml\n",
    "fsl_merge.inputs.tr = 2.0\n",
    "#fsl_merge.inputs.force_even = False     \n",
    "fsl_merge.inputs.ignore_exception = False     \n",
    "#fsl_merge.inputs.sort = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - low pass high pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces import afni\n",
    "\n",
    "bandpass = pe.Node(interface=afni.Bandpass(), name='bandpass')\n",
    "# bandpass.inputs.in_file = 'functional.nii'\n",
    "\n",
    "bandpass.inputs.highpass = 0.005\n",
    "bandpass.inputs.lowpass = 0.2\n",
    "bandpass.inputs.automask = False     \n",
    "bandpass.inputs.environ = {}\n",
    "bandpass.inputs.outputtype = 'NIFTI_GZ'     \n",
    "bandpass.inputs.terminal_output = 'stream'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connection du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "preproc.connect(norm12,  'normalized_files', fsl_merge, 'in_files')\n",
    "\n",
    "preproc.connect(fsl_merge,  'merged_file', bandpass, 'in_file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put the results in the datasink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasink = pe.Node(nio.DataSink(), name='datasink')\n",
    "datasink.inputs.base_directory = '/homes_unix/hirsch/_pypipe/datadir/data_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structural results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# segment results\n",
    "preproc.connect(segment,  'normalized_class_images', datasink, 'structural')\n",
    "preproc.connect(segment,  'bias_corrected_images' , datasink, 'structural.@par')\n",
    "preproc.connect(segment,  'bias_field_images' , datasink, 'structural.@par1')\n",
    "preproc.connect(segment,  'forward_deformation_field' , datasink, 'structural.@par2')\n",
    "preproc.connect(segment,  'inverse_deformation_field' , datasink, 'structural.@par3')\n",
    "preproc.connect(segment,  'modulated_class_images' , datasink, 'structural.mod_class')\n",
    "preproc.connect(segment,  'native_class_images' , datasink, 'structural.native_class')\n",
    "preproc.connect(segment,  'transformation_mat' , datasink, 'structural.@par6')\n",
    "preproc.connect(segment,  'dartel_input_images' , datasink, 'structural.@par7')\n",
    "\n",
    "# rajout de normalisation des tissues\n",
    "preproc.connect(norm12bis,  'normalized_files', datasink, 'structural.norm_files')\n",
    "preproc.connect(norm12bis,  'normalized_image', datasink, 'structural.norm_image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functional results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# slice timing results\n",
    "preproc.connect(st,  'timecorrected_files', datasink, 'functionnal')\n",
    "\n",
    "# realign mean_image and realignment_parameters\n",
    "preproc.connect(realign,  'mean_image', datasink, 'functionnal.@par')\n",
    "preproc.connect(realign,  'realignment_parameters', datasink, 'functionnal.@par1')\n",
    "preproc.connect(realign,  'realigned_files', datasink, 'functionnal.@par2')\n",
    "\n",
    "preproc.connect(coregister,  'coregistered_files', datasink, 'functionnal.@par3')\n",
    "\n",
    "preproc.connect(norm12,  'normalized_files', datasink, 'functionnal.norm_files')\n",
    "preproc.connect(norm12,  'normalized_image', datasink, 'functionnal.norm_image')\n",
    "\n",
    "\n",
    "\n",
    "preproc.connect(fsl_merge,  'merged_file', datasink, 'functionnal.@par6')\n",
    "\n",
    "preproc.connect(bandpass,  'out_file', datasink, 'functionnal.@par7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preproc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillons"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Node: fsconv.editWmMask     \n",
    "editWmMask = Node(MathsCommand(), name=\"editWmMask\")     \n",
    "editWmMask.inputs.args = '-thr 0 -uthr 111 -bin -ero -ero -ero '     \n",
    "editWmMask.inputs.ignore_exception = False     \n",
    "editWmMask.inputs.output_type = 'NIFTI_GZ'     \n",
    "editWmMask.inputs.terminal_output = 'stream'     \n",
    "fsconv.connect(fsWm2Nii, \"out_file\", editWmMask, \"in_file\")\n",
    "\n",
    "from web\n",
    "##############################################################################################\n",
    "##erode a mask or image by zeroing non-zero voxels when zero voxels found in kernel\n",
    "##############################################################################################\n",
    "fslmaths 'mask.nii.gz' -kernel box 5x5x5 -ero 'output_image.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arimav1.ipynb\r\n",
      "bandpassv1.ipynb\r\n",
      "bandpassv2.ipynb\r\n",
      "bandpassv3.ipynb\r\n",
      "bandpassv4.ipynb\r\n",
      "bandpassv4.py\r\n",
      "crash-20170309-124416-hirsch-norm12-3cfb0fd7-18cb-4810-ab7b-538590a2f7f8.pklz\r\n",
      "crash-20170314-121031-hirsch-bandpass-80dac15d-8ef0-47e0-8ece-c35f12d1fd6a.pklz\r\n",
      "crash-20170316-102433-hirsch-norm12bis-d218eac5-ec62-4fc1-94fd-7cd694989c71.pklz\r\n",
      "crash-20170316-183350-hirsch-datasink-2121ffc6-075d-4325-aed2-64824193f506.pklz\r\n",
      "crash-20170316-184003-hirsch-datasink-05c34874-77ed-4b0e-801f-1c7180916024.pklz\r\n",
      "crash-20170316-184615-hirsch-datasink-c8c536a8-d631-4ffc-8d2f-368af0d774d4.pklz\r\n",
      "crash-20170316-190454-hirsch-datasink-4a60535d-d75b-4c47-86ab-ccca48e2b2e9.pklz\r\n",
      "crash-20170317-105444-hirsch-computeMoco-352fa397-b097-46dd-92d8-a92264a119bb.pklz\r\n",
      "crash-20170317-105916-hirsch-computeMoco-b725d32b-af16-4ae5-a434-0dcce8facba0.pklz\r\n",
      "crash-20170317-114854-hirsch-computeMoco-0576aa30-4b34-4296-81fd-4a0fb0434464.pklz\r\n",
      "crash-20170317-120411-hirsch-computeMoco-faaf29ec-0e2a-4d22-a6d5-654f69e6f2a9.pklz\r\n",
      "crash-20170317-120924-hirsch-computeMoco-eb3c44ef-5adc-4e3a-b7d5-112f02dba0aa.pklz\r\n",
      "crash-20170317-121143-hirsch-computeMoco-13673e61-f82d-45cb-bc6a-42818b3cb593.pklz\r\n",
      "crash-20170317-121740-hirsch-datasink-0b512184-2feb-44ee-af60-2ea1586d77e9.pklz\r\n",
      "crash-20170320-193921-hirsch-computeStructuralImage-b6bd1f69-e652-42e4-8f0c-759c0173a259.pklz\r\n",
      "crash-20170321-103202-hirsch-computeStructuralImage-16ad5bed-bdee-43e8-b95e-e258179c033a.pklz\r\n",
      "nipypemjv1.ipynb\r\n",
      "nipypev1.ipynb\r\n",
      "nipypev2.ipynb\r\n",
      "nipypev3.ipynb\r\n",
      "nipypev3.py\r\n",
      "nipypev4.ipynb\r\n",
      "nipypev5(2).py\r\n",
      "nipypev5.ipynb\r\n",
      "nipypev6.ipynb\r\n",
      "nipypev7.ipynb\r\n",
      "nipypev7.py\r\n",
      "nipypev7-special-togetortho-v1.ipynb\r\n",
      "nipypev8.ipynb\r\n",
      "nipypev8.py\r\n",
      "ortho.txt\r\n",
      "\u001b[0m\u001b[38;5;27mprebandpass\u001b[0m/\r\n",
      "\u001b[38;5;27mpreproc\u001b[0m/\r\n",
      "pyscript.m\r\n",
      "try_nibabel_v1.ipynb\r\n",
      "try_nibabel_v2.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/homes_unix/hirsch/_ipnotebooks'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nipypev5.ipynb\n"
     ]
    }
   ],
   "source": [
    "path = u'/homes_unix/hirsch/_ipnotebooks'\n",
    "import os\n",
    "import re\n",
    "res = [f for f in os.listdir(path) if re.search(r'ni.*v5.ipynb', f)]\n",
    "for f in res:\n",
    "    print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'.ipynb_checkpoints',\n",
       " u'.git',\n",
       " u'nipypev1.ipynb',\n",
       " u'nipypev2.ipynb',\n",
       " u'nipypev3.ipynb',\n",
       " u'pyscript.m',\n",
       " u'preproc',\n",
       " u'nipypev3.py',\n",
       " u'nipypev4.ipynb',\n",
       " u'nipypev5.ipynb',\n",
       " u'nipypev5(2).py',\n",
       " u'try_nibabel_v2.ipynb',\n",
       " u'crash-20170321-103202-hirsch-computeStructuralImage-16ad5bed-bdee-43e8-b95e-e258179c033a.pklz',\n",
       " u'nipypev6.ipynb',\n",
       " u'crash-20170309-124416-hirsch-norm12-3cfb0fd7-18cb-4810-ab7b-538590a2f7f8.pklz',\n",
       " u'nipypev7.ipynb',\n",
       " u'crash-20170314-121031-hirsch-bandpass-80dac15d-8ef0-47e0-8ece-c35f12d1fd6a.pklz',\n",
       " u'nipypev7.py',\n",
       " u'arimav1.ipynb',\n",
       " u'nipypev7-special-togetortho-v1.ipynb',\n",
       " u'bandpassv1.ipynb',\n",
       " u'ortho.txt',\n",
       " u'nipypev8.ipynb',\n",
       " u'crash-20170316-102433-hirsch-norm12bis-d218eac5-ec62-4fc1-94fd-7cd694989c71.pklz',\n",
       " u'bandpassv2.ipynb',\n",
       " u'prebandpass',\n",
       " u'bandpassv3.ipynb',\n",
       " u'crash-20170316-183350-hirsch-datasink-2121ffc6-075d-4325-aed2-64824193f506.pklz',\n",
       " u'crash-20170316-184003-hirsch-datasink-05c34874-77ed-4b0e-801f-1c7180916024.pklz',\n",
       " u'crash-20170316-184615-hirsch-datasink-c8c536a8-d631-4ffc-8d2f-368af0d774d4.pklz',\n",
       " u'crash-20170316-190454-hirsch-datasink-4a60535d-d75b-4c47-86ab-ccca48e2b2e9.pklz',\n",
       " u'nipypev8.py',\n",
       " u'crash-20170317-105444-hirsch-computeMoco-352fa397-b097-46dd-92d8-a92264a119bb.pklz',\n",
       " u'crash-20170317-105916-hirsch-computeMoco-b725d32b-af16-4ae5-a434-0dcce8facba0.pklz',\n",
       " u'crash-20170317-114854-hirsch-computeMoco-0576aa30-4b34-4296-81fd-4a0fb0434464.pklz',\n",
       " u'crash-20170317-120411-hirsch-computeMoco-faaf29ec-0e2a-4d22-a6d5-654f69e6f2a9.pklz',\n",
       " u'crash-20170317-120924-hirsch-computeMoco-eb3c44ef-5adc-4e3a-b7d5-112f02dba0aa.pklz',\n",
       " u'crash-20170317-121143-hirsch-computeMoco-13673e61-f82d-45cb-bc6a-42818b3cb593.pklz',\n",
       " u'crash-20170317-121740-hirsch-datasink-0b512184-2feb-44ee-af60-2ea1586d77e9.pklz',\n",
       " u'bandpassv4.ipynb',\n",
       " u'bandpassv4.py',\n",
       " u'nipypemjv1.ipynb',\n",
       " u'try_nibabel_v1.ipynb',\n",
       " u'crash-20170320-193921-hirsch-computeStructuralImage-b6bd1f69-e652-42e4-8f0c-759c0173a259.pklz']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wraps command **fslmaths**\n",
      "\n",
      "\n",
      "Inputs::\n",
      "\n",
      "\t[Mandatory]\n",
      "\tin_file: (an existing file name)\n",
      "\t\timage to operate on\n",
      "\t\tflag: %s, position: 2\n",
      "\n",
      "\t[Optional]\n",
      "\targs: (a string)\n",
      "\t\tAdditional parameters to the command\n",
      "\t\tflag: %s\n",
      "\tenviron: (a dictionary with keys which are a value of type 'str' and\n",
      "\t\t with values which are a value of type 'str', nipype default value:\n",
      "\t\t {})\n",
      "\t\tEnvironment variables\n",
      "\tignore_exception: (a boolean, nipype default value: False)\n",
      "\t\tPrint an error message instead of throwing an exception in case the\n",
      "\t\tinterface fails to run\n",
      "\tinternal_datatype: ('float' or 'char' or 'int' or 'short' or 'double'\n",
      "\t\t or 'input')\n",
      "\t\tdatatype to use for calculations (default is float)\n",
      "\t\tflag: -dt %s, position: 1\n",
      "\tnan2zeros: (a boolean)\n",
      "\t\tchange NaNs to zeros before doing anything\n",
      "\t\tflag: -nan, position: 3\n",
      "\tout_file: (a file name)\n",
      "\t\timage to write\n",
      "\t\tflag: %s, position: -2\n",
      "\toutput_datatype: ('float' or 'char' or 'int' or 'short' or 'double'\n",
      "\t\t or 'input')\n",
      "\t\tdatatype to use for output (default uses input type)\n",
      "\t\tflag: -odt %s, position: -1\n",
      "\toutput_type: ('NIFTI_PAIR' or 'NIFTI_PAIR_GZ' or 'NIFTI_GZ' or\n",
      "\t\t 'NIFTI')\n",
      "\t\tFSL output type\n",
      "\tterminal_output: ('stream' or 'allatonce' or 'file' or 'none')\n",
      "\t\tControl terminal output: `stream` - displays to terminal immediately\n",
      "\t\t(default), `allatonce` - waits till command is finished to display\n",
      "\t\toutput, `file` - writes output to file, `none` - output is ignored\n",
      "\n",
      "Outputs::\n",
      "\n",
      "\tout_file: (an existing file name)\n",
      "\t\timage written after calculations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nipype.interfaces.fsl.maths import MathsCommand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MathsCommand().help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uses SPM12's new Normalise routine for warping an image to a template.\n",
      "Spatial normalisation is now done via the segmentation routine (which was\n",
      "known as ``New Segment`` in SPM8). Note that the normalisation in SPM12\n",
      "is done towards a file containing multiple tissue probability maps, which\n",
      "was not the cass in SPM8.\n",
      "\n",
      "http://www.fil.ion.ucl.ac.uk/spm/doc/manual.pdf#page=49\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import nipype.interfaces.spm as spm\n",
      ">>> norm12 = spm.Normalize12()\n",
      ">>> norm12.inputs.image_to_align = 'structural.nii'\n",
      ">>> norm12.inputs.apply_to_files = 'functional.nii'\n",
      ">>> norm12.run() # doctest: +SKIP\n",
      "\n",
      "Inputs::\n",
      "\n",
      "\t[Mandatory]\n",
      "\tdeformation_file: (a file name)\n",
      "\t\tfile y_*.nii containing 3 deformation fields for the deformation in\n",
      "\t\tx, y and z dimension\n",
      "\t\tmutually_exclusive: image_to_align, tpm\n",
      "\timage_to_align: (an existing file name)\n",
      "\t\tfile to estimate normalization parameters with\n",
      "\t\tmutually_exclusive: deformation_file\n",
      "\n",
      "\t[Optional]\n",
      "\taffine_regularization_type: ('mni' or 'size' or 'none')\n",
      "\t\tmni, size, none\n",
      "\tapply_to_files: (a list of items which are an existing file name or a\n",
      "\t\t list of items which are an existing file name)\n",
      "\t\tfiles to apply transformation to\n",
      "\tbias_fwhm: (30 or 40 or 50 or 60 or 70 or 80 or 90 or 100 or 110 or\n",
      "\t\t 120 or 130 or 140 or 150 or 'Inf')\n",
      "\t\tFWHM of Gaussian smoothness of bias\n",
      "\tbias_regularization: (0 or 1e-05 or 0.0001 or 0.001 or 0.01 or 0.1 or\n",
      "\t\t 1 or 10)\n",
      "\t\tno(0) - extremely heavy (10)\n",
      "\tignore_exception: (a boolean, nipype default value: False)\n",
      "\t\tPrint an error message instead of throwing an exception in case the\n",
      "\t\tinterface fails to run\n",
      "\tjobtype: ('estwrite' or 'est' or 'write', nipype default value:\n",
      "\t\t estwrite)\n",
      "\t\tEstimate, Write or do Both\n",
      "\tmatlab_cmd: (a string)\n",
      "\t\tmatlab command to use\n",
      "\tmfile: (a boolean, nipype default value: True)\n",
      "\t\tRun m-code using m-file\n",
      "\tpaths: (a list of items which are a directory name)\n",
      "\t\tPaths to add to matlabpath\n",
      "\tsampling_distance: (a float)\n",
      "\t\tSampling distance on data for parameter estimation\n",
      "\tsmoothness: (a float)\n",
      "\t\tvalue (in mm) to smooth the data before normalization\n",
      "\ttpm: (an existing file name)\n",
      "\t\ttemplate in form of tissue probablitiy maps to normalize to\n",
      "\t\tmutually_exclusive: deformation_file\n",
      "\tuse_mcr: (a boolean)\n",
      "\t\tRun m-code using SPM MCR\n",
      "\tuse_v8struct: (a boolean, nipype default value: True)\n",
      "\t\tGenerate SPM8 and higher compatible jobs\n",
      "\twarping_regularization: (a list of from 5 to 5 items which are a\n",
      "\t\t float)\n",
      "\t\tcontrols balance between parameters and data\n",
      "\twrite_bounding_box: (a list of from 2 to 2 items which are a list of\n",
      "\t\t from 3 to 3 items which are a float)\n",
      "\t\t3x2-element list of lists representing the bounding box (in mm) to\n",
      "\t\tbe written\n",
      "\twrite_interp: (0 <= an integer <= 7)\n",
      "\t\tdegree of b-spline used for interpolation\n",
      "\twrite_voxel_sizes: (a list of from 3 to 3 items which are a float)\n",
      "\t\t3-element list representing the voxel sizes (in mm) of the written\n",
      "\t\tnormalised images\n",
      "\n",
      "Outputs::\n",
      "\n",
      "\tdeformation_field: (a list of items which are an existing file name)\n",
      "\t\tNIfTI file containing 3 deformation fields for the deformation in x,\n",
      "\t\ty and z dimension\n",
      "\tnormalized_files: (a list of items which are an existing file name)\n",
      "\t\tNormalized other files\n",
      "\tnormalized_image: (a list of items which are an existing file name)\n",
      "\t\tNormalized file that needed to be aligned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spm.Normalize12().help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spm.Realign().help()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.tsa.arima_model.ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "afni.Bandpass().help()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels import  tsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spm\n",
    "\n",
    ".Normalize12().help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spm.Coregister.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spm.Realign().help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spm.Realign().input_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = epiResults['sliceTimingVector'].split()\n",
    "lint =[]\n",
    "for i in l:\n",
    "    lint.append(int(i))\n",
    "print lint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float(epiResults['TR']) - float(epiResults['TR'])/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spm.SliceTiming().help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spm.SliceTiming().input_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias_corrected_images = <undefined>\n",
    "bias_field_images = <undefined>\n",
    "dartel_input_images = <undefined>\n",
    "forward_deformation_field = <undefined>\n",
    "inverse_deformation_field = <undefined>\n",
    "modulated_class_images = <undefined>\n",
    "native_class_images = <undefined>\n",
    "normalized_class_images = <undefined>\n",
    "transformation_mat = <undefined>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spm.NewSegment.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spm.NewSegment.input_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "bias_corrected_images = <undefined>\n",
       "bias_field_images = <undefined>\n",
       "dartel_input_images = <undefined>\n",
       "forward_deformation_field = <undefined>\n",
       "inverse_deformation_field = <undefined>\n",
       "modulated_class_images = <undefined>\n",
       "native_class_images = <undefined>\n",
       "normalized_class_images = <undefined>\n",
       "transformation_mat = <undefined>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.NewSegment.output_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/Atlases/TPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd /homes_unix/hirsch/_pypipe/datadir/data_set/t0009/repos01/EPIBOLD\n",
    "%pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
